{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from gensim.models import FastText\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import cntk as C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Configuration\n",
    "\"\"\"\n",
    "embedding_path = '../embedding/fasttext_384_15_5_50_sg0.bin'\n",
    "train_data = '../data/train.ctf'\n",
    "valid_data = '../data/dev.ctf'\n",
    "test_data = 'test'\n",
    "valid_minibatch_size = 1024\n",
    "num_validation = 10000\n",
    "minibatch_size = 1024\n",
    "epoch_size = 4000000\n",
    "embedding_dim = 384\n",
    "learning_rate = 5\n",
    "hidden_dim = 256\n",
    "max_epochs = 300\n",
    "log_freq = 2000\n",
    "dropout = 0.2\n",
    "version = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CUDNN approach rnn stack\n",
    "\"\"\"\n",
    "from cntk.layers.blocks import _INFERRED\n",
    "def OptimizedRnnStack(hidden_dim, num_layers=1, recurrent_op='lstm', bidirectional=True, \n",
    "                      use_cudnn=True, name=''):\n",
    "    if use_cudnn:\n",
    "        W = C.parameter(_INFERRED + (hidden_dim,), init=C.glorot_uniform())\n",
    "        def func(x):\n",
    "            return C.optimized_rnnstack(x, W, hidden_dim, num_layers, bidirectional, \n",
    "                                        recurrent_op=recurrent_op, name=name)\n",
    "        return func\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step function used to calculate sequence length\n",
    "\"\"\"\n",
    "def plus1(x, y):\n",
    "    return x + 1 + (y - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Class\n",
    "\"\"\"\n",
    "from collections import defaultdict\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.embedding_path = embedding_path\n",
    "        self.fmodel = FastText.load(self.embedding_path)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_dim = len(self.fmodel.wv.vocab) + 1\n",
    "        self.vocab = defaultdict(int)\n",
    "        self.dropout = dropout\n",
    "        self.use_cudnn = True\n",
    "        \n",
    "        index = 1\n",
    "        for vocab in self.fmodel.wv.vocab:\n",
    "            self.vocab[vocab] = index\n",
    "            index += 1\n",
    "        \n",
    "    def embed(self):\n",
    "        vec = np.zeros((self.word_dim, self.embedding_dim), dtype=np.float32)\n",
    "\n",
    "        for vocab in self.fmodel.wv.vocab:\n",
    "            vec[self.vocab[vocab]] = self.fmodel.wv[vocab]\n",
    "        \n",
    "        embedding = C.parameter(shape=vec.shape, init=vec)\n",
    "        \n",
    "        def func(context):\n",
    "            return C.times(context, embedding)\n",
    "        \n",
    "        return func\n",
    "        \n",
    "    def input_layer(self, c1w, c2w):\n",
    "        c1w_ph = C.placeholder()\n",
    "        c2w_ph = C.placeholder()\n",
    "        \n",
    "        input_words = C.placeholder(shape=(self.word_dim))\n",
    "        \n",
    "        embedded = self.embed()(input_words)\n",
    "        processed = OptimizedRnnStack(self.hidden_dim,\n",
    "                       num_layers=1, bidirectional=True, use_cudnn=True, name='input_rnn')(embedded)\n",
    "        \n",
    "        c1_processed = processed.clone(C.CloneMethod.share, {input_words: c1w_ph})\n",
    "        c2_processed = processed.clone(C.CloneMethod.share, {input_words: c2w_ph})\n",
    "        \n",
    "        return C.as_block(\n",
    "            C.combine([c1_processed, c2_processed]),\n",
    "            [(c1w_ph, c1w), (c2w_ph, c2w)],\n",
    "            'input_layer',\n",
    "            'input_layer'\n",
    "        )\n",
    "    \n",
    "    def attention_layer(self, c1, c2, layer):        \n",
    "        q_processed = C.placeholder(shape=(2*self.hidden_dim,))\n",
    "        p_processed = C.placeholder(shape=(2*self.hidden_dim,))\n",
    "\n",
    "        qvw, qvw_mask = C.sequence.unpack(q_processed, padding_value=0).outputs\n",
    "\n",
    "        wq = C.parameter(shape=(2*self.hidden_dim, 2*self.hidden_dim), init=C.glorot_uniform())\n",
    "        wp = C.parameter(shape=(2*self.hidden_dim, 2*self.hidden_dim), init=C.glorot_uniform())\n",
    "        wg = C.parameter(shape=(8*self.hidden_dim, 8*self.hidden_dim), init=C.glorot_uniform())\n",
    "        v = C.parameter(shape=(2*self.hidden_dim, 1), init=C.glorot_uniform())\n",
    "\n",
    "        # seq[tensor[2d]] p_len x 2d\n",
    "        wpt = C.reshape(C.times(p_processed, wp), (-1, 2*self.hidden_dim))\n",
    "\n",
    "        # q_len x 2d\n",
    "        wqt = C.reshape(C.times(qvw, wq), (-1, 2*self.hidden_dim))\n",
    "        \n",
    "        # seq[tensor[q_len]]\n",
    "        S = C.reshape(C.times(C.tanh(C.sequence.broadcast_as(wqt, p_processed) + wpt), v), (-1))\n",
    "\n",
    "        qvw_mask_expanded = C.sequence.broadcast_as(qvw_mask, p_processed)\n",
    "\n",
    "        # seq[tensor[q_len]]\n",
    "        S = C.element_select(qvw_mask_expanded, S, C.constant(-1e+30))\n",
    "        \n",
    "        # seq[tensor[q_len]]\n",
    "        A = C.softmax(S, axis=0)\n",
    "\n",
    "        # seq[tensor[2d]]\n",
    "        swap_qvw = C.swapaxes(qvw)\n",
    "        cq = C.reshape(C.reduce_sum(A * C.sequence.broadcast_as(swap_qvw, A), axis=1), (-1))\n",
    "\n",
    "        # seq[tensor[4d]]\n",
    "        uc_concat = C.splice(p_processed, cq, p_processed * cq, cq * cq)\n",
    "        \n",
    "        # seq[tensor[4d]]\n",
    "        gt = C.tanh(C.times(uc_concat, wg))\n",
    "        \n",
    "        # seq[tensor[4d]]\n",
    "        uc_concat_star = gt * uc_concat\n",
    " \n",
    "        # seq[tensor[4d]]\n",
    "        vp = C.layers.Sequential([\n",
    "            C.layers.Dropout(self.dropout),\n",
    "            OptimizedRnnStack(self.hidden_dim, bidirectional=True, \n",
    "                use_cudnn=self.use_cudnn, name=layer+'_attention_rnn')])(uc_concat_star)\n",
    "        \n",
    "        return C.as_block(\n",
    "            vp,\n",
    "            [(p_processed, c1), (q_processed, c2)],\n",
    "            'attention_layer_' + layer,\n",
    "            'attention_layer_' + layer)\n",
    "        \n",
    "    def model(self):\n",
    "        c1_axis = C.Axis.new_unique_dynamic_axis('c1_axis')\n",
    "        c2_axis = C.Axis.new_unique_dynamic_axis('c2_axis')\n",
    "        b = C.Axis.default_batch_axis()\n",
    "        \n",
    "        c1 = C.input_variable(self.word_dim, dynamic_axes=[b, c1_axis], name='c1')\n",
    "        c2 = C.input_variable(self.word_dim, dynamic_axes=[b, c2_axis], name='c2')\n",
    "        \n",
    "        y = C.input_variable(1, dynamic_axes=[b], name='y')\n",
    "        \n",
    "        c1_processed, c2_processed = self.input_layer(c1, c2).outputs\n",
    "        att_context = self.attention_layer(c2_processed, c1_processed, 'attention')\n",
    "        \n",
    "        c2_len = C.layers.Fold(plus1)(c2_processed)\n",
    "        att_len = C.layers.Fold(plus1)(att_context)\n",
    "        \n",
    "        cos = C.cosine_distance(C.sequence.reduce_sum(c2_processed)/c2_len, \n",
    "                                C.sequence.reduce_sum(att_context)/att_len)\n",
    "        \n",
    "        prob = C.sigmoid(cos)\n",
    "        is_context = C.greater(prob, 0.5)\n",
    "        \n",
    "        loss = C.losses.binary_cross_entropy(prob, y)\n",
    "        acc = C.equal(is_context, y)\n",
    "        \n",
    "        return cos, loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read ctf format to minibatch source, input function\n",
    "\"\"\"\n",
    "def deserialize(func, ctf_path, model, randomize=True, repeat=True, is_test=False):\n",
    "    if not is_test:\n",
    "        mb_source = C.io.MinibatchSource(\n",
    "            C.io.CTFDeserializer(\n",
    "                ctf_path,\n",
    "                C.io.StreamDefs(\n",
    "                    c1 = C.io.StreamDef('c1', shape=model.word_dim, is_sparse=True),\n",
    "                    c2 = C.io.StreamDef('c2', shape=model.word_dim, is_sparse=True),\n",
    "                    y  = C.io.StreamDef('y', shape=1, is_sparse=False))),\n",
    "            randomize=randomize,\n",
    "            max_sweeps=C.io.INFINITELY_REPEAT if repeat else 1)\n",
    "\n",
    "        input_map = {\n",
    "            argument_by_name(func, 'c1'): mb_source.streams.c1,\n",
    "            argument_by_name(func, 'c2'): mb_source.streams.c2,\n",
    "            argument_by_name(func, 'y'): mb_source.streams.y\n",
    "        }\n",
    "    else:\n",
    "        mb_source = C.io.MinibatchSource(\n",
    "            C.io.CTFDeserializer(\n",
    "                ctf_path,\n",
    "                C.io.StreamDefs(\n",
    "                    c1 = C.io.StreamDef('c1', shape=model.word_dim, is_sparse=True),\n",
    "                    c2 = C.io.StreamDef('c2', shape=model.word_dim, is_sparse=True))),\n",
    "            randomize=randomize,\n",
    "            max_sweeps=C.io.INFINITELY_REPEAT if repeat else 1)\n",
    "\n",
    "        input_map = {\n",
    "            argument_by_name(func, 'c1'): mb_source.streams.c1,\n",
    "            argument_by_name(func, 'c2'): mb_source.streams.c2\n",
    "        }    \n",
    "    return mb_source, input_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper function used to map the variable\n",
    "\"\"\"\n",
    "def argument_by_name(func, name):\n",
    "    found = [arg for arg in func.arguments if arg.name == name]\n",
    "    if len(found) == 0:\n",
    "        raise ValueError('no matching names in arguments')\n",
    "    elif len(found) > 1:\n",
    "        raise ValueError('multiple matching names in arguments')\n",
    "    else:\n",
    "        return found[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Trainer\n",
    "\"\"\"\n",
    "def train():\n",
    "    model = Model()\n",
    "    z, loss, acc = model.model()\n",
    "    \n",
    "    progress_writers = [C.logging.ProgressPrinter(\n",
    "                            num_epochs = max_epochs,\n",
    "                            freq = log_freq,\n",
    "                            tag = 'Training',\n",
    "                            log_to_file = 'log/log_' + version)]\n",
    "    \n",
    "    lr = C.learning_parameter_schedule(learning_rate, minibatch_size=None, epoch_size=None)\n",
    "    learner = C.adadelta(z.parameters, lr)\n",
    "    trainer = C.Trainer(z, (loss, acc), learner, progress_writers)\n",
    "    \n",
    "    mb_source, input_map = deserialize(loss, train_data, model)\n",
    "    mb_valid, valid_map = deserialize(loss, valid_data, model)\n",
    "    \n",
    "    try:\n",
    "        trainer.restore_from_checkpoint('../model/' + version)\n",
    "    except Exception:\n",
    "        print('No checkpoint.')\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        num_seq = 0\n",
    "        # train\n",
    "        with tqdm_notebook(total=epoch_size) as progress_bar:\n",
    "            while True:\n",
    "                data = mb_source.next_minibatch(minibatch_size, input_map=input_map)\n",
    "                trainer.train_minibatch(data)\n",
    "                num_seq += trainer.previous_minibatch_sample_count\n",
    "                progress_bar.update(trainer.previous_minibatch_sample_count)\n",
    "                if num_seq >= epoch_size:\n",
    "                    break\n",
    "            trainer.summarize_training_progress()\n",
    "            trainer.save_checkpoint('../model/' + version + '/' + str(epoch)) \n",
    "        num_seq = 0        \n",
    "        # validation\n",
    "        with tqdm_notebook(total=num_validation) as valid_progress_bar:\n",
    "            while True:\n",
    "                data = mb_valid.next_minibatch(minibatch_size, input_map=valid_map)\n",
    "                if not data:\n",
    "                    break\n",
    "                trainer.test_minibatch(data)\n",
    "                num_seq += len(data)\n",
    "                valid_progress_bar.update(len(data))\n",
    "                if num_seq >= num_validation:\n",
    "                    break\n",
    "            trainer.summarize_test_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redirecting log to file log/log_train\n",
      "No checkpoint.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "014d993bb6ff4779b008518f7e36fd4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4000000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
